{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import model_selection # <---- PARTICIONADO\n",
    "\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "La función a continuación levanta todos los datos en la carpeta indicada (ej. \"clases/\") y los almacena en un diccionario, donde las claves corresponden a la clase en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_csv_files(folder, Nfeatures=102):\n",
    "    '''\n",
    "    Load all the \"CSV\" files in a folder, and store each one in a dictionary.\n",
    "    '''\n",
    "    \n",
    "    #filenames = glob.glob(rel_path + '*.csv')\n",
    "    \n",
    "    clase = dict()\n",
    "    \n",
    "    for n in range(0,24):\n",
    "        \n",
    "      clase[n] = pandas.read_csv(folder +'clase' + str(n) + '.csv', sep=',',header=None, names=['f'+str(m) for m in range(0,Nfeatures)])\n",
    "#        clase[n+1] = np.array(pandas.read_csv(folder +'clase' + str(n+1) + '.csv', sep=',',header=None, names=['f'+str(m) for m in range(0,Nfeatures)]))\n",
    "    return clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Levanto los datos de los cromosomas almacenados en la carpeta \"clases/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            f0        f1        f2        f3        f4        f5        f6  \\\n",
      "0     0.715849  1.000000  1.000000  0.865031  0.815951  0.588957  0.515337   \n",
      "1     0.570053  0.887097  0.862319  0.782609  0.695652  0.623188  0.442029   \n",
      "2     1.000000  1.000000  1.000000  0.854015  0.729927  0.591241  0.496350   \n",
      "3     0.961303  0.900000  0.968254  1.000000  0.833333  0.777778  0.706349   \n",
      "4     1.000000  1.000000  1.000000  0.904762  0.825397  0.698413  0.595238   \n",
      "5     0.566158  0.946565  1.000000  0.934307  0.868613  0.795620  0.583942   \n",
      "6     0.538017  0.918699  1.000000  0.815287  0.764331  0.656051  0.496815   \n",
      "7     1.000000  1.000000  1.000000  1.000000  0.784615  0.607692  0.584615   \n",
      "8     0.849269  1.000000  0.852941  0.705882  0.617647  0.582353  0.552941   \n",
      "9     0.502250  1.000000  1.000000  0.939189  0.810811  0.722973  0.702703   \n",
      "10    0.595556  0.989583  1.000000  0.910053  0.867725  0.798942  0.756614   \n",
      "11    0.423030  0.947917  1.000000  0.803030  0.727273  0.704545  0.681818   \n",
      "12    0.946919  1.000000  1.000000  1.000000  0.751938  0.674419  0.612403   \n",
      "13    0.823697  0.943182  1.000000  0.814516  0.717742  0.653226  0.564516   \n",
      "14    0.694636  0.954545  1.000000  1.000000  0.849673  0.738562  0.614379   \n",
      "15    0.699947  1.000000  1.000000  0.911111  0.807407  0.688889  0.548148   \n",
      "16    0.607202  1.000000  1.000000  0.904459  0.834395  0.713376  0.585987   \n",
      "17    0.656510  0.858333  1.000000  0.849315  0.719178  0.650685  0.650685   \n",
      "18    1.000000  1.000000  1.000000  0.852174  0.773913  0.652174  0.600000   \n",
      "19    0.312214  0.806723  0.881356  0.881356  0.677966  0.644068  0.711864   \n",
      "20    0.475648  1.000000  0.923529  0.870588  0.841176  1.000000  0.952941   \n",
      "21    0.460296  0.964286  1.000000  0.852349  0.684564  0.536913  0.489933   \n",
      "22    0.700184  0.913462  1.000000  0.897436  0.846154  0.769231  0.752137   \n",
      "23    0.558770  1.000000  1.000000  1.000000  0.858268  0.755906  0.787402   \n",
      "24    0.868764  0.819149  1.000000  0.806452  0.806452  0.685484  0.629032   \n",
      "25    0.959870  1.000000  1.000000  0.901099  0.675824  0.637363  0.571429   \n",
      "26    1.000000  0.990000  1.000000  0.867925  0.779874  0.716981  0.660377   \n",
      "27    0.578454  1.000000  1.000000  0.954545  0.846591  0.784091  0.715909   \n",
      "28    0.446698  0.900990  1.000000  0.852113  0.718310  0.669014  0.669014   \n",
      "29    1.000000  1.000000  1.000000  0.850746  0.791045  0.686567  0.574627   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1100  1.000000  1.000000  0.611111  0.436508  0.420635  0.412698  0.603175   \n",
      "1101  0.890837  0.898438  1.000000  0.921260  0.748031  0.590551  0.440945   \n",
      "1102  0.551512  0.951456  1.000000  0.864662  0.766917  0.751880  0.729323   \n",
      "1103  0.517486  1.000000  1.000000  0.948905  0.883212  0.788321  0.737226   \n",
      "1104  1.000000  1.000000  1.000000  1.000000  0.787234  0.709220  0.680851   \n",
      "1105  0.897302  0.957447  1.000000  0.936620  0.732394  0.619718  0.619718   \n",
      "1106  0.969027  0.954023  1.000000  0.803571  0.758929  0.660714  0.598214   \n",
      "1107  1.000000  1.000000  1.000000  0.893939  0.772727  0.681818  0.606061   \n",
      "1108  0.719265  0.627660  1.000000  0.927711  0.855422  0.819277  0.843373   \n",
      "1109  0.610111  1.000000  0.916667  0.916667  0.744792  0.645833  0.661458   \n",
      "1110  1.000000  0.936170  1.000000  0.921687  0.837349  0.680723  0.614458   \n",
      "1111  0.773268  0.971631  1.000000  0.951220  0.869919  0.756098  0.764228   \n",
      "1112  0.916236  1.000000  1.000000  0.938931  0.748092  0.641221  0.603053   \n",
      "1113  0.957601  0.901235  1.000000  0.836207  0.836207  0.706897  0.750000   \n",
      "1114  1.000000  0.960396  1.000000  0.907975  0.889571  0.809816  0.736196   \n",
      "1115  0.639729  1.000000  1.000000  0.896970  0.824242  0.781818  0.757576   \n",
      "1116  0.736605  0.989691  1.000000  1.000000  0.715385  0.646154  0.576923   \n",
      "1117  0.928893  1.000000  1.000000  0.951049  0.888112  0.818182  0.790210   \n",
      "1118  0.585161  0.875000  1.000000  0.965812  0.854701  0.777778  0.692308   \n",
      "1119  0.643226  0.977273  1.000000  0.825581  0.848837  0.813953  0.895349   \n",
      "1120  1.000000  0.956989  0.856250  1.000000  0.718750  0.506250  0.425000   \n",
      "1121  0.796671  1.000000  1.000000  0.762712  0.686441  0.618644  0.694915   \n",
      "1122  1.000000  0.946809  1.000000  0.845528  0.804878  0.682927  0.520325   \n",
      "1123  0.715724  0.936170  1.000000  1.000000  0.795276  0.645669  0.519685   \n",
      "1124  0.676126  0.968421  0.947368  0.885965  0.859649  0.842105  1.000000   \n",
      "1125  1.000000  0.915789  1.000000  1.000000  0.784722  0.604167  0.486111   \n",
      "1126  1.000000  1.000000  1.000000  0.905660  0.823899  0.735849  0.597484   \n",
      "1127  0.532338  1.000000  1.000000  0.895833  0.833333  0.805556  0.708333   \n",
      "1128  0.598827  0.881890  1.000000  0.837838  0.770270  0.702703  0.655405   \n",
      "1129  1.000000  1.000000  1.000000  0.897590  0.789157  0.644578  0.644578   \n",
      "\n",
      "            f7        f8        f9    ...          f92       f93       f94  \\\n",
      "0     0.466258  0.453988  0.466258    ...     0.134969  0.220859  0.361963   \n",
      "1     0.355072  0.289855  0.268116    ...     0.318841  0.333333  0.398551   \n",
      "2     0.386861  0.328467  0.306569    ...     0.328467  0.401460  0.416058   \n",
      "3     0.650794  0.611111  0.595238    ...     0.341270  0.404762  0.500000   \n",
      "4     0.460317  0.380952  0.460317    ...     0.357143  0.507937  0.515873   \n",
      "5     0.489051  0.496350  0.532847    ...     0.416058  0.496350  0.467153   \n",
      "6     0.426752  0.452229  0.458599    ...     0.560510  0.585987  0.585987   \n",
      "7     0.569231  0.615385  0.638462    ...     0.153846  0.292308  0.376923   \n",
      "8     0.547059  0.541176  0.558824    ...     0.564706  0.529412  0.494118   \n",
      "9     0.668919  0.668919  0.689189    ...     0.472973  0.581081  0.608108   \n",
      "10    0.703704  0.693122  0.671958    ...     0.492063  0.529101  0.513228   \n",
      "11    0.651515  0.628788  0.666667    ...     0.409091  0.598485  0.568182   \n",
      "12    0.620155  0.596899  0.589147    ...     0.418605  0.496124  0.480620   \n",
      "13    0.500000  0.564516  0.548387    ...     0.258065  0.266129  0.306452   \n",
      "14    0.673203  0.725490  0.758170    ...     0.477124  0.601307  0.640523   \n",
      "15    0.311111  0.237037  0.185185    ...     0.355556  0.703704  0.777778   \n",
      "16    0.566879  0.585987  0.605096    ...     0.216561  0.299363  0.503185   \n",
      "17    0.636986  0.643836  0.643836    ...     0.369863  0.458904  0.465753   \n",
      "18    0.660870  0.678261  0.582609    ...     0.617391  0.530435  0.556522   \n",
      "19    0.737288  0.754237  0.805085    ...     0.415254  0.627119  0.788136   \n",
      "20    0.776471  0.700000  0.588235    ...     0.347059  0.441176  0.488235   \n",
      "21    0.442953  0.536913  0.563758    ...     0.315436  0.315436  0.389262   \n",
      "22    0.726496  0.760684  0.769231    ...     0.452991  0.555556  0.598291   \n",
      "23    0.677165  0.598425  0.543307    ...     0.456693  0.488189  0.519685   \n",
      "24    0.629032  0.620968  0.612903    ...     0.395161  0.483871  0.491935   \n",
      "25    0.554945  0.543956  0.538462    ...     0.571429  0.659341  0.708791   \n",
      "26    0.635220  0.622642  0.610063    ...     0.383648  0.402516  0.433962   \n",
      "27    0.698864  0.664773  0.653409    ...     0.443182  0.579545  0.647727   \n",
      "28    0.626761  0.591549  0.549296    ...     0.281690  0.401408  0.570423   \n",
      "29    0.455224  0.462687  0.447761    ...     0.485075  0.567164  0.567164   \n",
      "...        ...       ...       ...    ...          ...       ...       ...   \n",
      "1100  0.666667  0.619048  0.690476    ...     0.515873  0.563492  0.650794   \n",
      "1101  0.385827  0.393701  0.464567    ...     0.196850  0.314961  0.519685   \n",
      "1102  0.699248  0.684211  0.646617    ...     0.315789  0.548872  0.631579   \n",
      "1103  0.591241  0.576642  0.562044    ...     0.671533  0.700730  0.700730   \n",
      "1104  0.609929  0.560284  0.510638    ...     0.226950  0.234043  0.248227   \n",
      "1105  0.556338  0.514085  0.485915    ...     0.316901  0.338028  0.323944   \n",
      "1106  0.607143  0.544643  0.553571    ...     0.294643  0.312500  0.357143   \n",
      "1107  0.568182  0.553030  0.560606    ...     0.439394  0.454545  0.439394   \n",
      "1108  0.837349  0.843373  0.837349    ...     0.500000  0.602410  0.644578   \n",
      "1109  0.661458  0.630208  0.645833    ...     0.083333  0.109375  0.125000   \n",
      "1110  0.530120  0.542169  0.500000    ...     0.379518  0.481928  0.548193   \n",
      "1111  0.772358  0.764228  0.739837    ...     0.292683  0.544715  0.617886   \n",
      "1112  0.526718  0.541985  0.557252    ...     0.290076  0.511450  0.618321   \n",
      "1113  0.750000  0.681034  0.637931    ...     0.275862  0.439655  0.577586   \n",
      "1114  0.668712  0.595092  0.607362    ...     0.392638  0.496933  0.509202   \n",
      "1115  0.727273  0.715152  0.721212    ...     0.460606  0.490909  0.515152   \n",
      "1116  0.546154  0.492308  0.500000    ...     0.615385  0.676923  0.723077   \n",
      "1117  0.748252  0.713287  0.734266    ...     0.314685  0.461538  0.573427   \n",
      "1118  0.623932  0.606838  0.589744    ...     0.367521  0.487179  0.495726   \n",
      "1119  0.930233  0.941860  0.918605    ...     0.255814  0.569767  0.732558   \n",
      "1120  0.393750  0.362500  0.337500    ...     0.275000  0.337500  0.337500   \n",
      "1121  0.728814  0.694915  0.601695    ...     0.127119  0.398305  0.398305   \n",
      "1122  0.560976  0.487805  0.447154    ...     0.349593  0.560976  0.560976   \n",
      "1123  0.519685  0.551181  0.629921    ...     0.480315  0.566929  0.582677   \n",
      "1124  0.894737  0.824561  0.807018    ...     0.438596  0.456140  0.464912   \n",
      "1125  0.409722  0.375000  0.340278    ...     0.444444  0.583333  0.659722   \n",
      "1126  0.559748  0.534591  0.534591    ...     0.509434  0.515723  0.534591   \n",
      "1127  0.694444  0.708333  0.673611    ...     0.527778  0.611111  0.666667   \n",
      "1128  0.689189  0.682432  0.601351    ...     0.709459  0.709459  0.716216   \n",
      "1129  0.614458  0.674699  0.668675    ...     0.506024  0.542169  0.554217   \n",
      "\n",
      "           f95       f96       f97       f98       f99      f100      f101  \n",
      "0     0.355828  0.312883  0.263804  0.233129  0.306748  0.447853  0.638037  \n",
      "1     0.333333  0.253623  0.137681  0.159420  0.333333  0.847826  1.000000  \n",
      "2     0.408759  0.437956  0.430657  0.430657  0.467153  0.518248  0.693431  \n",
      "3     0.500000  0.500000  0.436508  0.428571  0.587302  0.595238  0.698413  \n",
      "4     0.444444  0.357143  0.253968  0.333333  0.539683  0.730159  0.968254  \n",
      "5     0.474453  0.364964  0.386861  0.364964  0.306569  0.357664  0.343066  \n",
      "6     0.547771  0.490446  0.458599  0.471338  0.541401  0.700637  0.834395  \n",
      "7     0.569231  0.569231  0.461538  0.361538  0.338462  0.430769  0.761538  \n",
      "8     0.458824  0.411765  0.552941  0.935294  0.441176  1.000000  0.470588  \n",
      "9     0.614865  0.581081  0.554054  0.479730  0.479730  0.493243  0.527027  \n",
      "10    0.502646  0.470899  0.439153  0.417989  0.439153  0.439153  0.597884  \n",
      "11    0.401515  0.469697  0.492424  0.613636  0.696970  0.643939  0.674242  \n",
      "12    0.472868  0.465116  0.472868  0.465116  0.441860  0.511628  0.635659  \n",
      "13    0.354839  0.250000  0.290323  0.290323  0.258065  0.403226  0.451613  \n",
      "14    0.692810  0.581699  0.503268  0.398693  0.379085  0.516340  0.640523  \n",
      "15    0.814815  0.725926  0.622222  0.562963  0.562963  0.622222  0.762963  \n",
      "16    0.535032  0.522293  0.471338  0.382166  0.394904  0.515924  0.605096  \n",
      "17    0.465753  0.424658  0.397260  0.438356  0.630137  0.746575  0.883562  \n",
      "18    0.600000  0.600000  0.573913  0.513043  0.539130  0.626087  0.756522  \n",
      "19    0.847458  0.788136  0.627119  0.491525  0.389831  0.305085  0.338983  \n",
      "20    0.464706  0.488235  0.394118  0.364706  0.400000  0.464706  0.529412  \n",
      "21    0.402685  0.315436  0.315436  0.281879  0.302013  0.442953  0.624161  \n",
      "22    0.598291  0.547009  0.418803  0.435897  0.418803  0.452991  0.632479  \n",
      "23    0.488189  0.448819  0.614173  0.732283  0.582677  0.496063  0.637795  \n",
      "24    0.516129  0.483871  0.443548  0.475806  0.661290  0.862903  0.975806  \n",
      "25    0.703297  0.681319  0.648352  0.637363  0.659341  0.692308  0.802198  \n",
      "26    0.402516  0.364780  0.276730  0.427673  0.427673  0.553459  0.761006  \n",
      "27    0.676136  0.710227  0.698864  0.619318  0.573864  0.630682  0.676136  \n",
      "28    0.598592  0.556338  0.514085  0.429577  0.373239  0.394366  0.485915  \n",
      "29    0.582090  0.574627  0.559701  0.582090  0.634328  0.738806  0.820896  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1100  0.666667  0.531746  0.396825  0.301587  0.285714  0.650794  1.000000  \n",
      "1101  0.519685  0.496063  0.377953  0.330709  0.267717  0.559055  0.826772  \n",
      "1102  0.669173  0.661654  0.639098  0.571429  0.541353  0.511278  0.586466  \n",
      "1103  0.525547  0.437956  0.401460  0.401460  0.554745  0.766423  0.905109  \n",
      "1104  0.269504  0.283688  0.333333  0.397163  0.439716  0.539007  0.652482  \n",
      "1105  0.380282  0.380282  0.352113  0.274648  0.309859  0.443662  0.633803  \n",
      "1106  0.312500  0.205357  0.214286  0.205357  0.267857  0.383929  0.625000  \n",
      "1107  0.386364  0.348485  0.303030  0.287879  0.333333  0.424242  0.643939  \n",
      "1108  0.632530  0.590361  0.536145  0.433735  0.409639  0.367470  0.463855  \n",
      "1109  0.119792  0.177083  0.281250  0.307292  0.276042  0.223958  0.223958  \n",
      "1110  0.542169  0.457831  0.271084  0.240964  0.222892  0.319277  0.716867  \n",
      "1111  0.626016  0.528455  0.455285  0.252033  0.276423  0.382114  0.739837  \n",
      "1112  0.717557  0.702290  0.549618  0.503817  0.526718  0.557252  0.633588  \n",
      "1113  0.637931  0.594828  0.500000  0.224138  0.189655  0.258621  0.508621  \n",
      "1114  0.490798  0.453988  0.423313  0.337423  0.343558  0.368098  0.423313  \n",
      "1115  0.478788  0.484848  0.484848  0.484848  0.472727  0.557576  0.672727  \n",
      "1116  0.730769  0.707692  0.669231  0.576923  0.546154  0.592308  0.676923  \n",
      "1117  0.615385  0.580420  0.559441  0.433566  0.475524  0.538462  0.748252  \n",
      "1118  0.461538  0.410256  0.341880  0.247863  0.230769  0.256410  0.504274  \n",
      "1119  0.802326  0.779070  0.697674  0.558140  0.383721  0.383721  0.406977  \n",
      "1120  0.250000  0.275000  0.256250  0.206250  0.306250  0.450000  0.487500  \n",
      "1121  0.389831  0.338983  0.279661  0.237288  0.279661  0.406780  0.669492  \n",
      "1122  0.479675  0.430894  0.308943  0.317073  0.447154  0.609756  0.837398  \n",
      "1123  0.574803  0.472441  0.370079  0.338583  0.259843  0.338583  0.527559  \n",
      "1124  0.447368  0.403509  0.263158  0.219298  0.236842  0.429825  0.394737  \n",
      "1125  0.638889  0.583333  0.520833  0.500000  0.548611  0.611111  0.840278  \n",
      "1126  0.528302  0.471698  0.352201  0.377358  0.440252  0.679245  0.817610  \n",
      "1127  0.687500  0.618056  0.520833  0.465278  0.500000  0.604167  0.715278  \n",
      "1128  0.655405  0.635135  0.554054  0.554054  0.581081  0.675676  0.763514  \n",
      "1129  0.554217  0.518072  0.493976  0.451807  0.481928  0.620482  0.903614  \n",
      "\n",
      "[1130 rows x 102 columns]\n",
      "(1128, 102)\n",
      "(1128, 102)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clase = read_csv_files('clases/')\n",
    "\n",
    "print(clase[0])\n",
    "\n",
    "\n",
    "print(clase[1].shape)\n",
    "print(clase[2].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Genero el dataset que usaré para los experimentos. Para este ejemplo, voy a usar los cromosomas de la clase 1 y 2 solamente. Se podría extender para usar más cromosomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clases_a_usar = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for idx in clases_a_usar:\n",
    "    \n",
    "    x = np.array(clase[idx]).tolist()\n",
    "    y = idx * np.ones((clase[idx].shape[0]))\n",
    "    y = y.tolist()\n",
    "    \n",
    "    X.extend(x)\n",
    "    Y.extend(y)\n",
    "\n",
    "#print(X[0])\n",
    "#print(X[-1])\n",
    "#print(Y[0])\n",
    "#print(Y[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**NOTA**: Es má fácil manipular listas de número en lugar de arreglos numpy. Scikit-learn acepta tanto arreglos de numpy como listas de python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Particionado de los datos</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "TRAIN DATA\n",
      "[ 13.  11.  19. ...,   8.   9.   2.]\n",
      "-----------\n",
      "TEST DATA\n",
      "[ 12.  12.  13. ...,   3.  20.   3.]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN: 80% -- TEST: 20%\n",
    "#X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X,Y,test_size=0.4,random_state=0)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Ahora si convierto a NUMPY\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "\n",
    "print('-----------')\n",
    "print('TRAIN DATA')\n",
    "#print(X_train)\n",
    "print(Y_train)\n",
    "\n",
    "print('-----------')\n",
    "print('TEST DATA')\n",
    "#print (X_test)\n",
    "print (Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Preprocesamiento</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Estandarización</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3096.,  2673.,  2938.,  2717.,  3184.,  2003.,  1663.,   741.,\n",
       "          568.,  1205.]),\n",
       " array([-1.49972277, -1.11111285, -0.72250292, -0.333893  ,  0.05471692,\n",
       "         0.44332684,  0.83193676,  1.22054668,  1.6091566 ,  1.99776652,\n",
       "         2.38637645]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f447cd22048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)                    # Built scaler to normalize values (mean=0.0 and std=1.0)\n",
    "#scaler = preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X_train)  # Built scaler to scale values in range [Xmin,Xmax]\n",
    "#scaler = preprocessing.MaxAbsScaler().fit(X_train)                     # Built scaler to scale values in range [-1,1]\n",
    "\n",
    "# APPLY TRAINED SCALER\n",
    "sX_train = scaler.transform(X_train)\n",
    "\n",
    "sX_test = scaler.transform(X_test)\n",
    "\n",
    "sX_train.mean(axis=0)\n",
    "#sX_train.std(axis=0)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(np.array(X_train)[:,1],color='b')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.array(sX_train)[:,1],color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Normalización de los datos</h2>\n",
    "\n",
    "Revisar: *http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3078.,  2691.,  2938.,  2717.,  3184.,  2003.,  1684.,   720.,\n",
       "          568.,  1205.]),\n",
       " array([ 0.        ,  0.00149528,  0.00299055,  0.00448583,  0.0059811 ,\n",
       "         0.00747638,  0.00897166,  0.01046693,  0.01196221,  0.01345749,\n",
       "         0.01495276]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f447ccf50f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Previamente estandarizado (cada patrón tiene media=0 y varianza=1)\n",
    "normalizer = preprocessing.Normalizer(norm='l2', copy=True).fit(X_train.T)\n",
    "\n",
    "nX_train = normalizer.transform(X_train.T)\n",
    "nX_test = normalizer.transform(X_test.T)\n",
    "\n",
    "# Traspongo nuevamente\n",
    "nX_train = nX_train.T\n",
    "nX_test = nX_test.T\n",
    "\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(np.array(X_train)[:,1],color='b')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(np.array(sX_train)[:,1],color='r')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(np.array(nX_train)[:,1],color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Construcción del clasificador</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', gamma='auto', C=1, cache_size=500)\n",
    "#clf = svm.SVC(kernel='rbf', gamma=0.1, C=1000, cache_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Entrenamiento del clasificador</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=500, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAIN = X_train  # --> Datos CRUDOS\n",
    "TRAIN = sX_train # --> Datos ESTANDARIZADOS\n",
    "#TRAIN = nX_train # --> Datos NORMALIZADOS\n",
    "\n",
    "clf.fit(TRAIN, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Predicción y medidas de desempeño</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.89\n",
      "\n",
      "CONFUSSION MATRIX\n",
      "[[208   7   2   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  4 221   0   1   0   0   0   2   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   2   0]\n",
      " [  1   1 206   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  1   0   2 178  16   3   0   1   1   2   0   0   0   0   1   0   0   0\n",
      "    0   0   0   0   2   0]\n",
      " [  0   0   2  18 211   2   1   1   2   0   0   1   0   0   1   0   0   0\n",
      "    0   0   0   0   1   0]\n",
      " [  1   2   4   2   2 206   2   1   2   0   0   1   0   0   1   0   0   0\n",
      "    0   0   0   0   1   0]\n",
      " [  2   2   0   2   1   5 205   3   5   2   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   1   1 200  10   3   2   1   2   0   0   0   0   0\n",
      "    0   0   0   0   2   0]\n",
      " [  1   1   2   0   1   1   2   3 191   2   4   1   0   1   1   0   0   0\n",
      "    1   0   0   0   9   0]\n",
      " [  0   0   0   1   0   3   0   3   1 186   1   1   3   0   2   0   0   0\n",
      "    0   0   0   0   1   0]\n",
      " [  0   0   1   1   0   0   2   0   0   2 215   8   0   0   0   0   2   0\n",
      "    0   0   0   0   1   0]\n",
      " [  0   0   0   0   2   4   1   2   7   2   5 214   2   0   1   0   0   0\n",
      "    0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   1   0   4   0   0   1   0 209   2   3   7   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   4   3   3   1   1   8 187  11   0   0   2\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   1   1   0   0  11   8 204   3   0   4\n",
      "    0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   5   4   2 189   9  11\n",
      "    3   5   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   1   0   0   0   2   2   1   9 191   3\n",
      "    0   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   1   2   4  11   4 195\n",
      "    1   2   1   0   0   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   7\n",
      "  175   4   3  20   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   6   3   3\n",
      "   13 213   1   9   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    5   1 216  14   0   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   4\n",
      "   14   8  10 197   0   4]\n",
      " [  1   0   0   0   1   1   2   2  13   1   0   0   2   0   2   0   0   0\n",
      "    0   0   0   0 138   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   2\n",
      "    1   0   4   7   0  47]]\n",
      "\n",
      "\n",
      "Classification report for classifier SVC(C=1, cache_size=500, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.95      0.95       218\n",
      "        1.0       0.94      0.96      0.95       230\n",
      "        2.0       0.94      0.98      0.96       211\n",
      "        3.0       0.87      0.86      0.87       207\n",
      "        4.0       0.90      0.88      0.89       240\n",
      "        5.0       0.90      0.92      0.91       225\n",
      "        6.0       0.94      0.90      0.92       227\n",
      "        7.0       0.88      0.90      0.89       223\n",
      "        8.0       0.80      0.86      0.83       221\n",
      "        9.0       0.91      0.92      0.91       202\n",
      "       10.0       0.94      0.93      0.93       232\n",
      "       11.0       0.94      0.89      0.91       241\n",
      "       12.0       0.85      0.92      0.89       227\n",
      "       13.0       0.90      0.85      0.88       220\n",
      "       14.0       0.86      0.87      0.86       234\n",
      "       15.0       0.84      0.83      0.83       229\n",
      "       16.0       0.91      0.88      0.89       217\n",
      "       17.0       0.84      0.87      0.86       224\n",
      "       18.0       0.82      0.82      0.82       213\n",
      "       19.0       0.88      0.84      0.86       253\n",
      "       20.0       0.92      0.90      0.91       239\n",
      "       21.0       0.80      0.82      0.81       239\n",
      "       22.0       0.87      0.85      0.86       163\n",
      "       23.0       0.80      0.76      0.78        62\n",
      "\n",
      "avg / total       0.89      0.89      0.89      5197\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TEST = X_test  # --> Datos CRUDOS\n",
    "TEST = sX_test # --> Datos ESTANDARIZADOS\n",
    "#TEST = nX_test # --> Datos NORMALIZADOS\n",
    "\n",
    "#======================\n",
    "# SCORE CALCULATION\n",
    "#======================\n",
    "score = clf.score(TEST, Y_test)\n",
    "\n",
    "print(\"\\nAccuracy: %0.2f\" % (score))\n",
    "\n",
    "\n",
    "\n",
    "#===================\n",
    "# CONFUSSION MATRIX\n",
    "#===================\n",
    "Y_pred = clf.predict(TEST)\n",
    "\n",
    "print('\\nCONFUSSION MATRIX')\n",
    "print(metrics.confusion_matrix(Y_test,Y_pred))\n",
    "\n",
    "\n",
    "#=========\n",
    "# REPORT\n",
    "#=========\n",
    "print(\"\\n\\nClassification report for classifier %s:\\n\\n%s\\n\"\n",
    "      %\n",
    "      (clf, metrics.classification_report(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante ver que en la diagonal de la matriz de confusión están los valores más altos (se confunde relativamente poco el clasificador)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
